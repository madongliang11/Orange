1.爬虫项目的搭建:
    1.安装scrapy框架 pip install scrapy
    2.安装Twisted库 pip install Twisted文件路径/Twisted文件的全称(注意:Twisted文件最好添加环境变量中,放入python解释器安装路径下是一个不错的选择)
    3.部分无法正常运行的用户需要加载pypiwin32库  pip install pypiwin32
    4.创建爬虫项目: scrapy startproject 项目名称
    5.创建爬虫: scrapy genspider 爬虫名称 爬虫所在域(例如:www.jd.com) 注意:不要加https协议
                scrapy genspider -t crawl 爬虫名称 爬虫所在域
    #########当完成以上的工作,爬虫项目就算搭建成功了

2.爬虫项目所需要的第三方库:
    pip install sqlalchemy
    pip install pillow
    pip install pymysql
    pip install redis
    pip install selenium

3.爬虫进行爬取数据的过程:
    1.项目开始启动的时候 (本项目中to_scrapy_main.py文件启动程序,进行项目测试),引擎自动的扫描配置文件-settings.py文件,并读取spider上的start_url,
        第一个需要加载的页面的url,并将其加入调度器的队列中
    2.引擎从调度器中取出需要下载的url,将其发送给下载器,令其下载该页面
    3.下载器下载完成后,引擎获取从下载器返回的response,将response发送给spider
    3.spider将对response进行解析,获取想要的数据
    4.spider若解析到需要下载的页面的url,则将url发送给引擎,引擎则重复以上步骤
    5.spider若是解析到需要数据,则将数据打包成item对象,并将该对象发送给引擎,引擎将接收到的item对象发送给管道pipeline,最终pipeline将对数据进行处理

